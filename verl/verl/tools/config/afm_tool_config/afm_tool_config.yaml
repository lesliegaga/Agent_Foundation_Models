tools:
  # code executor
  - class_name: verl.tools.code_executor.CodeExecutor
    config:
      retrieval_service_url: Local
      num_workers: 120
      memory_limit: 10*1024**3  # MB
      timeout: 30   # second
      nsjail_path: "/home/tongyan.zjy/workspace/git/nsjail/nsjail"   # path to the nsjail sandbox executable file
      max_obs_length: 512   # str-level observation len limit
    tool_schema:  # OpenAIFunctionToolSchema
      type: function
      function:   # OpenAIFunctionSchema
        name: code
        description: Execute the code blob in nsjail sandbox and return corresponding results.
        parameters:   # OpenAIFunctionParametersSchema
          type: object
          properties:   # Dict[str, OpenAIFunctionParametersSchema]
            query:
              type: string
              description: The code blob written by LLMs
          required: ["query"]

  # web search - basic search func
  - class_name: verl.tools.web_search_v2.SerperCacheAPITool
    config:
      timeout: 60
      num_results: 10
    tool_schema:
      type: function
      function:
        name: web_search
        description: Search the web using Google via Serper API with caching. Returns search results with title, link, and snippet.
        parameters:
          type: object
          properties:
            query:
              type: string
              description: The search query
            num_results:
              type: integer
              description: Number of search results to return (default 10)
              default: 10
          required: ["query"]

  # crawl page v2 - crawl pages and summary by LLM
  - class_name: verl.tools.crawl_page_v2.CrawlPageV2Tool
    config:
      timeout: 500
      summary_type: "page"
      chunk_size: 8192
      do_last_summary: false
    tool_schema:
      type: function
      function:
        name: crawl_page
        description: Crawl web pages and get AI-powered summary of their content. Use this after web_search to get detailed information from specific pages.
        parameters:
          type: object
          properties:
            query:
              type: string
              description: URLs to crawl, separated by '|'. You can use URLs from web_search results. Supports plain URLs or markdown format '[title](url)'
            summary_type:
              type: string
              description: Type of summarization - 'none', 'once', 'chunk', or 'page' (default 'once')
              enum: ["none", "once", "chunk", "page"]
              default: "once"
            chunk_size:
              type: integer
              description: Size of chunks for chunk-based summarization (default 8192)
              default: 8192
            do_last_summary:
              type: boolean
              description: Whether to do final summary for multi-chunk/page results (default false)
              default: false
          required: ["query"]

  # visual inspector - describe an img using LLMs
  - class_name: verl.tools.visual_inspector.VisualInspector
    config:
      model_name: gpt-4o   # Inference model name
      text_limit: 100000
      download_path: experiments/mm_data    # Relative path of the url download and local imgs
    tool_schema:  # OpenAIFunctionToolSchema
      type: function
      function:   # OpenAIFunctionSchema
        name: visual_inspector
        description: Process image files or web image URLs to get descriptions or answer questions about them. Cannot load files directly.
        parameters:   # OpenAIFunctionParametersSchema
          type: object
          properties:   # Dict[str, OpenAIFunctionParametersSchema]
            query:
              type: string
              description: File path or web image URL (e.g., 'https://example.com/image.jpg') to be read as an image. Must be in supported image formats (.jpg/.jpeg/.png/.gif/.bmp/.webp).
          required: ["query"]